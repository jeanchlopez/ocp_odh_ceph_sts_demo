{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "\n",
    "## User 2 has to add the ARN value of the role created by user-1\n",
    "\n",
    "Role_Arn = 'arn:aws:iam::odh1:role/S3Access'\n",
    "\n",
    "\n",
    "odh2_user1_sts = boto3.client('sts',\n",
    "           aws_access_key_id='odh2user1',\n",
    "           aws_secret_access_key='odh2user1pass',\n",
    "           endpoint_url='http://ceph-rgw.odh-1.svc.cluster.local',\n",
    "           region_name='')\n",
    "\n",
    "\n",
    "response = odh2_user1_sts.assume_role(\n",
    "            RoleArn=Role_Arn,\n",
    "            RoleSessionName='session_odh2user1',\n",
    "            DurationSeconds=3600\n",
    "            )\n",
    "\n",
    "\n",
    "aws_access_key_id = response['Credentials']['AccessKeyId']\n",
    "aws_secret_access_key = response['Credentials']['SecretAccessKey']\n",
    "endpoint_url='http://ceph-rgw.odh-1.svc.cluster.local'\n",
    "\n",
    "s3client = boto3.client('s3',\n",
    "            aws_access_key_id = response['Credentials']['AccessKeyId'],\n",
    "            aws_secret_access_key = response['Credentials']['SecretAccessKey'],\n",
    "            aws_session_token = response['Credentials']['SessionToken'],\n",
    "            endpoint_url='http://ceph-rgw.odh-1.svc.cluster.local',\n",
    "            region_name='',)\n",
    "\n",
    "bucket_name = 'ufo-dataset'\n",
    "#s3bucket = s3client.create_bucket(Bucket=bucket_name)\n",
    "#print(s3client.list_buckets())\n",
    "\n",
    "print(\"Getting files from odh1user1 dataset \"+ bucket_name +\" bucket, without sharing credentials \\n\")\n",
    "for key in s3client.list_objects(Bucket=bucket_name)['Contents']:\n",
    "    print(key['Key'])\n",
    "    \n",
    "#print(\"\\n Creating a new bucket from odh2user1 account\")\n",
    "#try:\n",
    "#    s3bucket = s3client.create_bucket(Bucket='odh2user1-bucket')\n",
    "#except ClientError as e:\n",
    "#    print(\"\\n Bucket already exists \\n\")\n",
    "\n",
    "# Retrieve the list of existing buckets\n",
    "#response = s3client.list_buckets()\n",
    "\n",
    "# Output the bucket names\n",
    "#print('Existing buckets:')\n",
    "#for bucket in response['Buckets']:\n",
    "#    print(f'  {bucket[\"Name\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"UFO_Analysis_using_SPARK\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_SSL = 'False'\n",
    "\n",
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.access.key\", aws_access_key_id)\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", aws_secret_access_key)\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", endpoint_url)\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", S3_SSL)\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "ceph_object_storage_dataset_path = \"s3a://\"+bucket_name+\"/UFO_dataset_kaggle.csv\"\n",
    "\n",
    "df0 = spark.read.csv(ceph_object_storage_dataset_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Qestion - 1 : What are the TOP-5 countries which reported UFO sighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1 = df0.groupBy(\"country\").count().toPandas()\n",
    "import plotly \n",
    "plotly.tools.set_credentials_file(username='xxx', api_key='xxxx')\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "data1 = [go.Bar(\n",
    "    x=plot1['country'],\n",
    "    y=plot1['count'],\n",
    "    width = 0.8 \n",
    ")]\n",
    "py.iplot(data1, filename='basic-bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import os\n",
    "import socket\n",
    "\n",
    "# create a spark session\n",
    "spark_cluster_url = f\"spark://{os.environ['SPARK_CLUSTER']}:7077\"\n",
    "spark = SparkSession.builder.master(spark_cluster_url).getOrCreate()\n",
    "\n",
    "# test your spark connection\n",
    "spark.range(5, numPartitions=5).rdd.map(lambda x: socket.gethostname()).distinct().collect()\n",
    "\n",
    "data = spark.read.csv('https://gitlab.com/opendatahub/opendatahub.io/-/raw/master/assets/files/tutorials/basic/sample_data.csv', sep=\",\", header=True)\n",
    "df = data.toPandas()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
